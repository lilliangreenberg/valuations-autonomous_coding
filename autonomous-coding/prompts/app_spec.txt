<project_specification>
<project_name>Company Valuation Intelligence Tool</project_name>

<overview>
Build a CLI application for large investment vehicles to automatically monitor portfolio companies and detect
status changes requiring human review. The system processes company data from Airtable, scrapes website content,
and searches for recent news articles to identify companies that may have closed, been acquired, or experienced
significant negative changes. The tool maintains historical snapshots for change detection and flags companies
requiring analyst attention based on multiple signals including website changes, news keyword detection, dead
websites, stale content, and acquisition mentions. Essential batch processing capabilities enable efficient
monitoring of 50-500+ companies per run with parallel execution, progress tracking, and robust error handling.
</overview>

<technology_stack>
<core_technologies>
<python>Python 3.12+</python>
<package_manager>uv for all package management</package_manager>
<database>SQLite with proper schema migrations</database>
<architecture>Functional Core / Imperative Shell (FC/IS) pattern</architecture>
</core_technologies>

<external_services>
  <airtable>Data source for company names and URLs from Portfolio Companies and Online Presence sheets</airtable>
  <firecrawl>Web scraping service for most resources (Homepage, Blog, Twitter, etc.) - handles rate limiting</firecrawl>
  <playwright>REQUIRED for LinkedIn scraping (both Corporate and Person) - Firecrawl cannot access LinkedIn</playwright>
</external_services>

<web_scraping>
  <firecrawl>
    - Primary scraping engine for most websites with built-in rate limiting
    - CRITICAL: MUST use start_batch_scrape endpoint (NOT batch_scrape)
    - Batch size: exactly 20 companies at a time (strict limit)
    - Captures HTML and Markdown content
    - Optional screenshot capture (configurable per URL, especially for Twitter)
    - Handles JavaScript rendering automatically
  </firecrawl>

  <firecrawl_search>
    - Use search API for finding news articles about companies
    - Query pattern: "Find articles about [Company Name] ([business description]) within the last 90 days. Prioritize articles from reputable sources. For each article identified, return the URL and the date it was published."
    - Requires: company name and business description (to be added to companies table)
    - Returns: article URLs and publication dates
  </firecrawl_search>

  <playwright>
    - REQUIRED for ALL LinkedIn scraping (Corporate and Person profiles) - Firecrawl cannot access LinkedIn
    - Used for URLs flagged with requires_playwright=true (JavaScript-heavy sites)
    - Adaptive fallback: Try Firecrawl first, switch to Playwright on failure, flag URL for future
    - Supports screenshot capture when needed
    - Implement delays to avoid detection/blocking
  </playwright>

  <scraper_selection_logic>
    - LinkedIn resources: Always use Playwright (hard requirement)
    - URLs with requires_playwright=true: Use Playwright
    - All other URLs: Try Firecrawl first
    - On Firecrawl failure: Switch to Playwright, set requires_playwright=true for future runs
    - Learning approach: Build list of problematic domains over time
  </scraper_selection_logic>

  <beautifulsoup4>HTML parsing for content extraction and link discovery</beautifulsoup4>
  <requests>HTTP client with timeout and retry configuration (for non-Firecrawl requests)</requests>
</web_scraping>

<text_analysis>
  <keyword_detection>Search for specific keywords and phrases in news and website content</keyword_detection>
  <text_preprocessing>Clean HTML, extract meaningful text, normalize for analysis</text_preprocessing>
  <string_matching>Simple substring matching for status indicators (acquisitions, closures, etc.)</string_matching>
  <ocr_processing>Extract text from screenshots using OCR (Tesseract) for keyword analysis - especially important for Twitter/LinkedIn image posts</ocr_processing>
</text_analysis>

<data_validation>
  <pydantic>Pydantic v2 for dataclasses and validation</pydantic>
  <url_validation>Validate and normalize URLs</url_validation>
  <duplicate_resolution>Intelligent deduplication of company names across Airtable sheets with canonical name retrieval</duplicate_resolution>

 <json_schema>JSON schema for structure checking</json_schema>
</data_validation>

<development_tools>
  <orm>SQLAlchemy v2 for storage, Alembic for schema management and migrations</orm>
  <linting>ruff for linting and formatting</linting>
  <type_checking>mypy in strict mode</type_checking>
  <logging>standard python logging library</logging>
  <testing>pytest with 90% minimum coverage</testing>
  <bdd_testing>behave for BDD-style integration tests</bdd_testing>
  <browser_automation>playwright for LinkedIn scraping (REQUIRED - Firecrawl cannot access LinkedIn)</browser_automation>
  <ocr>Tesseract OCR or similar for extracting text from screenshots (especially Twitter/LinkedIn image posts)</ocr>
</development_tools>

<cli_framework>
  <framework>click or typer for CLI interface (to be selected during implementation)</framework>
  <progress>rich for progress bars, tables, and formatted output</progress>
  <json_export>jq-compatible JSON output for pipeline integration</json_export>
</cli_framework>

</technology_stack>

<prerequisites>
<environment_setup>
- Python 3.12+ installed
- uv package manager configured
- Airtable API key with read access to company sheets
- Firecrawl API key with sufficient quota
- Write permissions for SQLite database location
- Internet connectivity for scraping and news search
</environment_setup>

<project_structure>
  - Follows src/ layout pattern (mandatory)
  - Functional Core / Imperative Shell pattern to clearly separate business logic from support
  - XDG Base Directory standards for file locations
  - Comprehensive .gitignore for standard file types
</project_structure>

<api_credentials>
  - AIRTABLE_API_KEY: Environment variable for Airtable access
  - AIRTABLE_BASE_ID: Base ID for company data
  - FIRECRAWL_API_KEY: Environment variable for Firecrawl service
  - Configuration file support for non-sensitive settings
</api_credentials>

</prerequisites>

<core_features>
<airtable_integration>
- Connect to two Airtable sheets: 'Portfolio Companies' and 'Online Presence'
- Extract canonical company names from 'Portfolio Companies' sheet (column: company name)
- Extract URLs from 'Online Presence' sheet with columns:
  - 'company_name': linked field to Portfolio Companies
  - 'Resource': type of link (23 types supported)
  - 'url': the actual URL
- Supported Resource types:
  - Homepage, Blog, Twitter, LinkedIn - Corporate, LinkedIn - Person
  - Crunchbase, Bloomberg, Bluesky, EDGAR, Wikipedia
  - Github, YCombinator, Facebook, Instagram, Tiktok
  - Reddit, Youtube, State Business Registry
  - Google query, Kagi query, No/little info / Unsure, Other
- Build many-to-many relationships: Companies ←→ CompanyURLs ←→ URLs
- Handle companies with many URLs, few URLs, or zero URLs
- Create company records with status='no_urls' and has_urls=false for companies without URLs
- Validate URL formats and clean data for each resource type
- Read-only access: never write back to Airtable (all updates in SQLite)
- Batch fetch optimization to minimize API calls
- Error handling for missing, malformed, or inaccessible URLs
- Deduplicate URLs across the database (same URL = same row in urls table)
</airtable_integration>

<website_snapshot_system>
  <crawling>
    - Scrape all company URLs (homepages, social media, LinkedIn pages) using Firecrawl
    - Capture both HTML and Markdown representations
    - Follow links to discover related pages (About Us, Blog, etc.) in the same domain
    - Configurable depth limit for page discovery (default: 2 levels)
    - Note pages forbidden by robots.txt
    - Handle JavaScript-rendered content
    - Detect and flag paywalled content
    - Detect sites requiring authentication
  </crawling>

  <content_storage>
    - Store full HTML content in database
    - Store cleaned Markdown content for analysis
    - Track HTTP status codes and response headers
    - Capture Last-Modified and ETag headers
    - Store HTTP redirects and final URLs
    - Indefinite retention for historical comparison
    - Retain a database of historical content for search
    - Detect duplicates to avoid storing multiple copies of the same page
  </content_storage>

  <retry_logic>
    - 2 retry attempts per company (as specified)
    - Exponential backoff between retries (2s, 4s, 8s)
    - Handle DNS failures, timeouts, connection errors
    - Log failed companies for manual review
    - Continue batch processing despite individual failures
  </retry_logic>

  <special_case_detection>
    - Identify 404 errors and DNS failures → Flag as Likely Closed
    - Detect paywall indicators (content truncation, subscription prompts)
    - Identify authentication requirements (login pages, 401/403 errors)
    - Detect redirects to parking pages or domain sale pages
    - Recognize maintenance mode pages
    - Limit number of pages to be retrieved from a single site (default: 10), set per site
  </special_case_detection>

  <multi_link_handling>
    - Process all resource types associated with each company (23 types supported)
    - Handle companies with zero URLs gracefully (status='no_urls', skip scraping)
    - Handle companies with multiple URLs efficiently (parallel processing per company)
    - **Triage approach:** ANY negative signal from ANY source → flag as "requires_review"
    - Better to over-flag than miss something requiring human judgment

    <scraping_strategy>
      - CRITICAL: Use Playwright for ALL LinkedIn resources (LinkedIn - Corporate and LinkedIn - Person)
      - Firecrawl CANNOT access LinkedIn - this will cause failures if not addressed
      - Use Firecrawl for most other resources (Homepage, Twitter, Blog, etc.)
      - Use Playwright as fallback if Firecrawl encounters issues with social media platforms
      - Rate limiting handled by Firecrawl; Playwright should implement delays
    </scraping_strategy>

    <signal_priority>
      - ANY negative signal triggers "requires_review" status
      - No weighting needed - single strong signal is enough for human review
      - Conflicting signals (e.g., dead homepage but active Twitter) → automatically flag for review
      - Social media activity = positive operational signal
      - Social media inactivity alone = neutral (not a problem)
      - Social media inactivity + other negative signals = flag for review
    </signal_priority>

    <platform_specific_handling>
      - Homepage: Check for 404, DNS failure, parking pages, redirects to acquirer
      - Twitter/X: Check for account suspension, deletion, name changes, recent activity
      - LinkedIn - Corporate: Check for company page existence, deactivation, recent updates
      - LinkedIn - Person: Check for role changes (e.g., CEO on website now at different company = closure signal)
      - Instagram/Facebook: Check for page removal, privacy changes, recent activity
      - Blog: Check for recent posts (within 3 months = operational signal)
      - Wikipedia/Crunchbase: Check for closure/acquisition mentions
      - EDGAR: Check for bankruptcy filings, dissolution notices
      - Maximum 2 employee profiles (LinkedIn - Person) per company - check all
    </platform_specific_handling>

    <status_determination>
      - Track verification status per URL individually
      - Aggregate all signals across all URLs for company-wide status
      - If ALL URLs are dead/inaccessible → high confidence "likely_closed"
      - If SOME URLs dead + other negative signals → "requires_review"
      - If conflicting signals (some active, some dead) → "requires_review"
      - If no negative signals detected → "operational"
      - Goal: identify companies needing human review, not make final determinations
    </status_determination>
  </multi_link_handling>
</website_snapshot_system>

<snapshot_comparison_engine>
  <change_detection>
    - MD5 checksum comparison between current and previous snapshots
    - Compare text content only (strip CSS, styling, and cosmetic changes)
    - Normalize whitespace and formatting before comparison
    - Compare HTTP Last-Modified headers across snapshots
    - Detect content additions, removals, and modifications
    - Identify structural changes (page removed, new pages added)
    - Track change frequency for each URL
  </change_detection>

  <change_classification>
    - Minor: <10% content change
    - Moderate: 10-30% content change
    - Major: >30% content change
    - Calculate percentage based on text content diff (ignore styling)
    - All page sections weighted equally (no special handling for footer, contact, etc.)
    - Threshold configuration via settings
    - Any detected change triggers review flag (triage approach)
  </change_classification>

  <change_history>
    - Only save snapshots where text content has changed from last snapshot
    - If content is unchanged, log successful retrieval but don't store duplicate snapshot
    - Maintain only most recent snapshot (assuming changes detected)
    - Timestamp each snapshot with capture time
    - Provide diff views for changed content (text only)
    - Generate change reports for analysis
    - Track what changed, not just that it changed
  </change_history>

  <cosmetic_handling>
    - Strip out CSS, JavaScript, styling before comparison
    - Ignore whitespace differences (normalize)
    - Ignore HTML structure changes if text content identical
    - Focus on actual content changes only
    - Don't flag purely visual updates
  </cosmetic_handling>
</snapshot_comparison_engine>

<status_analysis_system>
  <dead_website_detection>
    - DNS resolution failures → Likely Closed (high confidence)
    - 404 errors on homepage → Likely Closed (high confidence)
    - 5xx errors persisting across retries → Likely Closed (medium confidence)
    - Expired SSL certificates → Operational but flagged for review
    - Redirect to domain parking page → Likely Closed
    - Successful retrieval but content substantially changed → Operational but flagged for review (might be a domain name sale to a different company)
  </dead_website_detection>

  <acquisition_detection>
    - Search page content for acquisition keywords:
      - "acquired by", "acquisition", "merged with"
      - "now part of", "joined forces with"
      - "purchased by", "bought by"
    - Check for prominent acquisition announcements
    - Detect redirects to acquirer's website
    - Flag as Likely Closed with acquisition note
  </acquisition_detection>

  <staleness_detection>
    - Extract copyright year from footer
    - Pre-2024 copyright → Likely Closed (medium confidence)
    - Pre-2023 copyright → Likely Closed (high confidence)
    - Check blog post dates (if present)
    - Analyze Last-Modified header age
    - No updates in 12+ months → Flag for review
  </staleness_detection>

  <operational_signals>
    - Recent content updates (within 3 months) → Operational
    - Active blog or news section → Operational
    - Recent job postings → Operational
    - Customer testimonials or case studies → Operational
    - Contact forms and active support → Operational
  </operational_signals>

  <confidence_scoring>
    - Calculate confidence score (0.0-1.0) for status determination
    - Combine multiple signal sources (dead site, stale content, acquisitions)
    - Weight signals by reliability
    - Flag uncertain cases (<0.7 confidence) for manual review
    - Provide explanation for each status determination
  </confidence_scoring>
</status_analysis_system>

<news_monitoring_system>
  <news_search>
    - Use Firecrawl search API to find news articles about companies
    - Query format: "Find articles about [Company Name] ([business description]) within the last 90 days. Prioritize articles from reputable sources. For each article identified, return the URL and the date it was published."
    - Time window: Past 90 days (configurable)
    - Requires: company.name and company.business_description (to be added)
    - Returns: article URLs and publication dates
    - Scrape returned article URLs using Firecrawl batch scraping
    - Deduplicate articles across multiple companies (same URL can mention multiple companies)
    - Extract article title, content, URL, publication date
    - Handle company name variations through business description context
  </news_search>

  <keyword_analysis>
    - Search article content for specific positive and negative keywords
    - Count keyword matches per category (positive vs negative)
    - Weight recent articles more heavily
    - Simple scoring based on keyword presence and frequency
  </keyword_analysis>

  <positive_signals>
    - Funding/Investment: "funding", "investment", "raised", "series A", "series B", "series C", "series D", "seed round", "angel investment", "venture capital", "VC backing", "oversubscribed", "unicorn", "valuation", "pre-money", "post-money", "term sheet"
    - Product/Launch: "launch", "launched", "new product", "product release", "announcing", "unveiling", "beta", "public release", "GA", "general availability"
    - Growth/Success: "revenue", "growth", "profitable", "profitability", "milestone", "record sales", "market share", "customer growth", "ARR", "MRR", "doubled", "tripled", "breakthrough"
    - Partnerships: "partnership", "collaboration", "agreement", "strategic alliance", "joint venture", "signed deal", "partnership with", "teamed up"
    - Expansion: "expansion", "expanding", "new office", "new location", "international", "hiring", "jobs", "recruiting", "adding staff", "headcount", "scale up", "scaling"
    - Recognition: "award", "recognition", "winner", "top 10", "best of", "innovation award", "named to list", "industry leader"
    - IPO/Exit: "IPO", "going public", "public offering", "SPAC", "direct listing"
    → Store in database, positive signal detected
  </positive_signals>

  <negative_signals>
    - Closure: "closed", "closing", "shutdown", "shut down", "ceased operations", "ceasing operations", "out of business", "discontinued", "winding down", "liquidation", "closing doors"
    - Layoffs/Downsizing: "layoff", "layoffs", "laid off", "restructuring", "downsizing", "workforce reduction", "cutting staff", "job cuts", "letting go", "terminated", "RIF", "reduction in force", "furlough"
    - Financial Distress: "bankruptcy", "bankrupt", "insolvent", "insolvency", "financial distress", "Chapter 11", "Chapter 7", "administration", "receivership", "cash crunch", "running out of money", "burn rate", "runway", "going concern", "down round", "bridge round" (context dependent)
    - Legal Issues: "lawsuit", "sued", "legal action", "regulatory action", "investigation", "probe", "subpoena", "complaint", "litigation", "settlement", "fine", "penalty", "violation"
    - Security/Breach: "breach", "security incident", "hack", "hacked", "compromised", "data leak", "cyberattack", "ransomware", "vulnerability"
    - Acquisition: "acquired by", "acquisition", "bought by", "purchased by", "merged with", "merger", "takeover", "sold to", "acquires" (note: check if subject or object)
    - Leadership Changes: "CEO resigned", "CEO departure", "CFO resigned", "founder left", "executive departure", "stepping down", "ousted", "fired"
    - Product Failures: "recall", "failed", "discontinued product", "cancelled", "pulled from market", "defect", "malfunction"
    - Market Exit: "exiting market", "pulling out", "retreat", "abandoned", "pivoting away"
    → Flag company as Requires Human Attention
  </negative_signals>

  <keyword_matching_rules>
    - Minimum 2 keyword matches required to flag an article as positive/negative
    - Track presence only (not frequency) - found once = counted
    - If article has 2+ positive AND 2+ negative keywords → signal_type = "mixed" → flag for review
    - If article has 2+ positive keywords only → signal_type = "positive"
    - If article has 2+ negative keywords only → signal_type = "negative" → flag for review
    - If article has <2 keywords total → signal_type = "neutral" → not flagged
    - Store all matched keywords in matched_keywords field for transparency
  </keyword_matching_rules>

  <no_news_handling>
    - If no news articles found, do not adjust company status
    - Log "no news found" for historical record
    - Continue with website-based analysis only
  </no_news_handling>
</news_monitoring_system>

<batch_processing>
  <essential_requirements>
    - Process companies in batches of exactly 20 at a time (strict Firecrawl requirement)
    - Support processing large portfolios (50-500+ companies total) by running multiple 20-company batches
    - Use Firecrawl start_batch_scrape endpoint for each batch of 20
    - Parallel processing within each batch (configurable concurrency, default: 10 workers)
    - Rate limiting handled by Firecrawl for batch scraping
    - Progress tracking with real-time updates (per batch and overall)
    - Estimated time remaining calculation
    - Checkpointing to resume interrupted runs (track which batches completed)
  </essential_requirements>

  <execution_strategy>
    - Queue-based processing with priority handling
    - Worker pool for parallel HTTP requests
    - Graceful shutdown on interruption (Ctrl+C)
    - Save partial results on failure
    - Resume capability from last successful checkpoint
    - Skip companies already processed within the last day (configurable)
  </execution_strategy>

  <error_handling>
    - Continue processing on individual company failures
    - Aggregate errors for end-of-run report
    - Categorize errors: network, API, parsing, validation
    - Retry failed companies at end of batch
    - Generate error log file for debugging
    - Send notifications for critical failures
  </error_handling>

  <progress_reporting>
    - Real-time progress bar (using rich library)
    - Live statistics: processed, failed, flagged
    - Companies per second throughput
    - ETA for batch completion
    - Memory and CPU usage monitoring
    - Detailed logging to file
  </progress_reporting>
</batch_processing>

<cli_interface>
  <main_command>
    # Process companies from Airtable
    valuation-tool process

    # Process with options
    valuation-tool process --concurrency 20 --skip-news --force-refresh

    # Dry run to preview what would be processed
    valuation-tool process --dry-run
  </main_command>

  <query_commands>
    # List all companies requiring human attention
    valuation-tool query flagged

    # Show detailed status for specific company
    valuation-tool query company "Acme Corp"

    # Show all links for a specific company
    valuation-tool query company-links "Acme Corp"

    # List companies by status
    valuation-tool query by-status likely-closed

    # List companies with no links
    valuation-tool query no-links

    # List companies with dead links
    valuation-tool query dead-links

    # Show recent changes
    valuation-tool query changes --since 7d

    # Search companies
    valuation-tool query search "tech startup"
  </query_commands>

  <export_commands>
    # Export flagged companies to JSON
    valuation-tool export --format json --status flagged --output results.json

    # Export to CSV
    valuation-tool export --format csv --output results.csv

    # Export with filters
    valuation-tool export --format json --changed-since 2024-01-01 --confidence-min 0.8
  </export_commands>

  <utility_commands>
    # Show database statistics
    valuation-tool stats

    # Verify Airtable and Firecrawl connections
    valuation-tool config check

    # Initialize or migrate database
    valuation-tool db init
    valuation-tool db migrate

    # Clear old snapshots (keep latest N)
    valuation-tool db clean --keep 5
  </utility_commands>

  <configuration>
    - Global configuration file: ~/.config/valuation-tool/config.toml
    - Environment variables for sensitive credentials
    - Command-line flags override configuration
    - Verbose and quiet modes for output control
    - Debug mode with detailed logging
  </configuration>
</cli_interface>

<output_formats>
  <cli_summary_report>
    - Formatted table of flagged companies
    - Status breakdown (Operational, Likely Closed, Requires Review)
    - Key statistics (total processed, flagged, errors)
    - Highlight companies with high-confidence negative signals
    - Color-coded output (green=operational, red=flagged, yellow=uncertain)
    - Exportable to terminal or file
  </cli_summary_report>

  <json_export>
    - Structured JSON following consistent schema
    - Company details, status, confidence scores
    - Change history and snapshots
    - News articles and keyword match data
    - Timestamps and metadata
    - jq-compatible for command-line processing
  </json_export>

  <csv_export>
    - Flat CSV format for spreadsheet import
    - Columns: Company Name, URL, Status, Confidence, Last Checked, Flags
    - Optional detailed columns with change descriptions
    - Compatible with Excel and Google Sheets
  </csv_export>

  <database_storage>
    - Primary data store in SQLite
    - Normalized schema for efficiency
    - Full historical data retention
    - Query-optimized indexes
    - Supports concurrent reads
  </database_storage>
</output_formats>

<logging_and_monitoring>
  - Standard python logging
  - Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
  - Separate log files by run with timestamps
  - Contextual information (company name, URL, operation)
  - Performance metrics (timing, API calls, success rates)
  - Error stack traces for debugging
  - Rotation and archival of old logs
</logging_and_monitoring>

</core_features>

<database_schema>
<companies>
- id: INTEGER PRIMARY KEY
- name: TEXT NOT NULL UNIQUE
- airtable_record_id: TEXT (from Portfolio Companies sheet)
- naics_code: TEXT (industry classification code - to be added)
- business_description: TEXT (what the company does - to be added, used for news search)
- status: TEXT (operational, likely_closed, requires_review, unknown, no_urls)
- confidence_score: REAL (0.0-1.0)
- has_urls: BOOLEAN (false if no URLs in Airtable)
- last_checked_at: TIMESTAMP
- first_seen_at: TIMESTAMP
- created_at: TIMESTAMP
- updated_at: TIMESTAMP
- notes: TEXT (e.g., "Acquired by Company X", "404 error", "No URLs available")
</companies>

<urls>
  - id: INTEGER PRIMARY KEY
  - url: TEXT NOT NULL UNIQUE
  - resource_type: TEXT (Homepage, Blog, Twitter, LinkedIn - Corporate, LinkedIn - Person, Crunchbase, Bloomberg, Bluesky, EDGAR, Wikipedia, Github, YCombinator, Facebook, Instagram, Tiktok, Reddit, Youtube, State Business Registry, Google query, Kagi query, No/little info / Unsure, Other)
  - airtable_record_id: TEXT (source record from Online Presence sheet)
  - requires_playwright: BOOLEAN (true if JavaScript-heavy or Firecrawl fails, LinkedIn always true)
  - capture_screenshot: BOOLEAN (true if screenshots needed, especially for Twitter)
  - discovered_at: TIMESTAMP
  - last_verified_at: TIMESTAMP
  - is_active: BOOLEAN (true if link is currently valid/accessible)
  - verification_status: TEXT (valid, dead, redirected, error, not_checked)
  - http_status: INTEGER (last known HTTP status code)
  - scraper_used: TEXT (firecrawl, playwright)
  - notes: TEXT (e.g., "redirects to parent company", "404 error", "Firecrawl failed, switched to Playwright")
  - created_at: TIMESTAMP
  - updated_at: TIMESTAMP
</urls>

<company_urls>
  - id: INTEGER PRIMARY KEY
  - company_id: INTEGER FOREIGN KEY REFERENCES companies(id)
  - url_id: INTEGER FOREIGN KEY REFERENCES urls(id)
  - is_primary: BOOLEAN (true for main homepage of this company)
  - relationship_notes: TEXT (why this URL relates to this company)
  - created_at: TIMESTAMP
  - UNIQUE(company_id, url_id) (prevent duplicate relationships)
</company_urls>

<website_snapshots>
  - id: INTEGER PRIMARY KEY
  - url_id: INTEGER FOREIGN KEY REFERENCES urls(id)
  - captured_at: TIMESTAMP
  - final_url: TEXT (final URL after redirects, if different from original)
  - http_status: INTEGER
  - html_content: TEXT
  - markdown_content: TEXT
  - content_checksum: TEXT (MD5 hash)
  - screenshot_path: TEXT (optional - path to screenshot file if captured)
  - ocr_text: TEXT (text extracted from screenshot via OCR, if screenshot captured)
  - last_modified_header: TEXT
  - etag: TEXT
  - is_paywall: BOOLEAN
  - requires_auth: BOOLEAN
  - scraper_used: TEXT (firecrawl, playwright)
  - error_message: TEXT (if scraping failed)
  - metadata: JSON (additional headers, redirect chain, batch info, etc.)
</website_snapshots>

<discovered_pages>
  - id: INTEGER PRIMARY KEY
  - snapshot_id: INTEGER FOREIGN KEY
  - url: TEXT
  - page_type: TEXT (about, blog, contact, etc.)
  - title: TEXT
  - content_checksum: TEXT
  - captured_at: TIMESTAMP
</discovered_pages>

<snapshot_changes>
  - id: INTEGER PRIMARY KEY
  - url_id: INTEGER FOREIGN KEY REFERENCES urls(id)
  - old_snapshot_id: INTEGER FOREIGN KEY REFERENCES website_snapshots(id)
  - new_snapshot_id: INTEGER FOREIGN KEY REFERENCES website_snapshots(id)
  - change_magnitude: TEXT (minor, moderate, major)
  - change_percentage: REAL
  - detected_at: TIMESTAMP
  - change_summary: TEXT (description of changes)
</snapshot_changes>

<news_articles>
  - id: INTEGER PRIMARY KEY
  - url_id: INTEGER FOREIGN KEY REFERENCES urls(id)
  - title: TEXT
  - content: TEXT
  - published_at: TIMESTAMP
  - discovered_at: TIMESTAMP
  - source: TEXT (news outlet name)
  - positive_keyword_count: INTEGER (count of positive keywords found)
  - negative_keyword_count: INTEGER (count of negative keywords found)
  - signal_type: TEXT (positive, negative, neutral, mixed)
  - matched_keywords: JSON (list of matched keywords for transparency)
  - is_relevant: BOOLEAN (false if off-topic)
  - notes: TEXT
</news_articles>

<news_article_note>
  Note: News articles relate to companies through the company_urls join table.
  A single news article (stored in urls table with resource_type='news') can mention
  multiple companies, and the company_urls table tracks these relationships.
</news_article_note>

<status_determinations>
  - id: INTEGER PRIMARY KEY
  - company_id: INTEGER FOREIGN KEY
  - determined_at: TIMESTAMP
  - status: TEXT (operational, likely_closed, requires_review)
  - confidence_score: REAL (0.0-1.0)
  - signals: JSON (list of signals contributing to determination)
  - explanation: TEXT (human-readable reasoning)
</status_determinations>

<processing_runs>
  - id: INTEGER PRIMARY KEY
  - started_at: TIMESTAMP
  - completed_at: TIMESTAMP
  - companies_processed: INTEGER
  - companies_flagged: INTEGER
  - companies_failed: INTEGER
  - status: TEXT (in_progress, completed, failed, interrupted)
  - configuration: JSON (settings used for this run)
  - error_log: TEXT
</processing_runs>

<airtable_sync>
  - id: INTEGER PRIMARY KEY
  - sync_type: TEXT (companies, urls)
  - airtable_sheet: TEXT (Portfolio Companies, Online Presence)
  - airtable_record_id: TEXT
  - entity_id: INTEGER (company_id or url_id depending on sync_type)
  - last_synced_at: TIMESTAMP
  - sync_direction: TEXT (import - read-only for now)
  - status: TEXT (success, failed)
  - error_message: TEXT
  - records_processed: INTEGER
</airtable_sync>

</database_schema>

<data_models>
<company>
- name: str
- airtable_record_id: str | None
- naics_code: str | None (industry classification - to be added)
- business_description: str | None (what company does - to be added, used for news search)
- status: Literal["operational", "likely_closed", "requires_review", "unknown", "no_urls"]
- confidence_score: float (range: 0.0-1.0)
- has_urls: bool
- last_checked_at: datetime | None
- notes: str | None
- urls: list[URL] (related URLs via company_urls join table)
</company>

<url>
  - url: HttpUrl (validated URL)
  - resource_type: Literal["Homepage", "Blog", "Twitter", "LinkedIn - Corporate", "LinkedIn - Person", "Crunchbase", "Bloomberg", "Bluesky", "EDGAR", "Wikipedia", "Github", "YCombinator", "Facebook", "Instagram", "Tiktok", "Reddit", "Youtube", "State Business Registry", "Google query", "Kagi query", "No/little info / Unsure", "Other"]
  - airtable_record_id: str | None
  - requires_playwright: bool (true if JS-heavy or Firecrawl fails, LinkedIn always true)
  - capture_screenshot: bool (true if screenshots needed, especially Twitter)
  - discovered_at: datetime
  - last_verified_at: datetime | None
  - is_active: bool
  - verification_status: Literal["valid", "dead", "redirected", "error", "not_checked"]
  - http_status: int | None
  - scraper_used: Literal["firecrawl", "playwright"] | None
  - notes: str | None
  - companies: list[Company] (related companies via company_urls join table)
</url>

<company_url>
  - company_id: int
  - url_id: int
  - is_primary: bool (true for main homepage of this company)
  - relationship_notes: str | None
  - created_at: datetime
</company_url>

<website_snapshot>
  - url_id: int
  - captured_at: datetime
  - final_url: HttpUrl | None (if redirected)
  - http_status: int
  - html_content: str
  - markdown_content: str
  - content_checksum: str
  - screenshot_path: str | None (path to screenshot file if captured)
  - ocr_text: str | None (text extracted from screenshot via OCR)
  - last_modified: datetime | None
  - etag: str | None
  - is_paywall: bool
  - requires_auth: bool
  - scraper_used: Literal["firecrawl", "playwright"]
  - error_message: str | None
</website_snapshot>

<news_article>
  - url_id: int (references urls table)
  - title: str
  - content: str
  - published_at: datetime
  - source: str
  - positive_keyword_count: int
  - negative_keyword_count: int
  - signal_type: Literal["positive", "negative", "neutral", "mixed"]
  - matched_keywords: list[str]
  - is_relevant: bool
  - companies: list[Company] (related via company_urls join table)
  - notes: str | None
</news_article>

<status_determination>
  - company_id: int
  - determined_at: datetime
  - status: Literal["operational", "likely_closed", "requires_review"]
  - confidence_score: float (range: 0.0-1.0)
  - signals: list[StatusSignal]
  - explanation: str
</status_determination>

<status_signal>
  - signal_type: str (e.g., "dead_website", "stale_copyright", "negative_news")
  - weight: float (importance of this signal)
  - value: Any (signal-specific data)
  - description: str
</status_signal>

<snapshot_change>
  - company_id: int
  - old_snapshot_id: int
  - new_snapshot_id: int
  - change_magnitude: Literal["minor", "moderate", "major"]
  - change_percentage: float
  - detected_at: datetime
  - change_summary: str
</snapshot_change>

<processing_result>
  - company: Company
  - status_determination: StatusDetermination | None
  - snapshots_captured: int
  - news_articles_found: int
  - changes_detected: int
  - processing_time: float (seconds)
  - errors: list[str]
</processing_result>

<batch_summary>
  - run_id: int
  - started_at: datetime
  - completed_at: datetime
  - total_companies: int
  - companies_processed: int
  - companies_flagged: int
  - companies_failed: int
  - processing_time: float
  - results: list[ProcessingResult]
</batch_summary>

</data_models>

<system_architecture>
<high_level_design>
┌─────────────────────┐
│   CLI Interface     │
│  - Commands         │
│  - Argument parsing │
└──────────┬──────────┘
│
▼
┌─────────────────────┐
│  Batch Processor    │
│  - Queue management │
│  - Parallel workers │
│  - Progress tracking│
└──────────┬──────────┘
│
┌───────┴───────┐
▼               ▼
┌─────────┐    ┌──────────┐
│ Airtable│    │ Database │
│ Fetcher │    │ Manager  │
└────┬────┘    └─────┬────┘
│               │
▼               ▼
┌─────────────────────┐
│ Company Processor   │
│ - Orchestrates flow │
└──────────┬──────────┘
│
┌───────┼───────┐
▼       ▼       ▼
┌──────┐┌──────┐┌────────┐
│Website││ News ││ Status │
│Scraper││Search││Analyzer│
└───┬───┘└───┬──┘└────┬───┘
│       │        │
▼       ▼        ▼
┌─────────────────────┐
│   Keyword           │
│   Detector          │
└──────────┬──────────┘
│
▼
┌─────────────────────┐
│  Results Formatter  │
│  - JSON/CSV export  │
│  - CLI reports      │
└─────────────────────┘
</high_level_design>

<fc_is_pattern>
  Imperative Shell                   Functional Core
  ┌──────────────────────┐          ┌──────────────────┐
  │ CLI commands         │─────────>│ Company validation│
  │ Airtable API calls   │─────────>│ URL normalization │
  │ Firecrawl API calls  │─────────>│ Keyword matching  │
  │ Database I/O         │─────────>│ Change detection  │
  │ File exports         │<─────────│ Status logic      │
  │ Progress display     │<─────────│ Signal aggregation│
  └──────────────────────┘          └──────────────────┘
     Side Effects                      Pure Functions
     (src/valuation_tool/              (src/valuation_tool/
      infrastructure/)                  core/)
</fc_is_pattern>

<processing_flow>
  1. CLI invoked with "process" command
  2. Load configuration from file and environment
  3. Initialize database connection and migrations
  4. Fetch company list from Airtable (with deduplication)
  5. Build processing queue with priority ordering
  6. Initialize worker pool for parallel processing
  7. For each company (in parallel):
     a. Fetch or create company record in database
     b. Check if recent snapshot exists (skip if fresh and --force-refresh not set)
     c. Scrape homepage and related pages using Firecrawl
     d. Store raw HTML and Markdown content
     e. Search for news articles in past 2 weeks
     f. Apply keyword detection to news content (positive/negative signals)
     g. Compare new snapshot to historical snapshots (MD5 checksums)
     h. Detect dead websites, stale content, acquisitions
     i. Calculate confidence score for status determination
     j. Update company status in database
     k. Log processing result
  8. Aggregate results from all workers
  9. Generate CLI summary report
  10. Export to JSON/CSV if requested
  11. Display final statistics and flagged companies
  12. Log batch completion
</processing_flow>

<error_recovery>
  - Graceful degradation on API failures
  - Skip company on repeated failures (log and continue)
  - Checkpoint progress every 10 companies
  - Resume from checkpoint on interruption
  - Detailed error logging for debugging
  - Retry transient failures (network timeouts, 5xx errors)
  - No retry for permanent failures (404, invalid URLs)
</error_recovery>

</system_architecture>

<design_principles>
<quality_over_speed>
- Reliability and data accuracy prioritized
- Proper error handling mandatory
- Thorough testing required (90% coverage minimum)
- Clear, maintainable code
- Comprehensive logging with structlog
</quality_over_speed>

<no_backward_compatibility>
  - Breaking changes acceptable during development
  - Always use best current approach
  - Delete old code completely (no deprecation)
  - Refactor aggressively
  - Use latest Python features and patterns
</no_backward_compatibility>

<solid_principles>
  - Single Responsibility: One reason to change
  - Open/Closed: Open for extension, closed for modification
  - Liskov Substitution: Derived classes substitutable
  - Interface Segregation: No fat interfaces
  - Dependency Inversion: Depend on abstractions
</solid_principles>

<test_driven_development>
  - Tests written alongside implementation
  - 90% minimum line coverage
  - 100% coverage for functional core (business logic)
  - Integration tests for external APIs (mocked)
  - BDD tests with behave for user scenarios (optional)
</test_driven_development>

<user_experience>
  - Clear progress indicators during batch processing
  - Helpful error messages with actionable guidance
  - Sensible defaults requiring minimal configuration
  - Fast feedback loops with dry-run mode
  - Professional output formatting with rich library
  - Interruptible processing (Ctrl+C safe)
</user_experience>

<data_integrity>
  - Atomic database transactions
  - Validation at system boundaries (Airtable input, API responses)
  - Checksums for content integrity
  - Audit trail for all status changes
  - Safe deletion (soft delete or archive)
</data_integrity>

</design_principles>

<implementation_steps>
<step number="1">
<title>Project Foundation and Database</title>
<tasks>
- Set up src/ layout project structure
- Configure uv package management (pyproject.toml)
- Establish core/ and infrastructure/ separation (FC/IS)
- Configure mypy strict mode, ruff, pytest
- Create base Pydantic models
- Set up logging configuration (standard Python logging)
- Initialize .gitignore with standard patterns
- Design SQLite schema with migrations:
  - companies table (with has_urls flag, status including 'no_urls')
  - urls table (with 23 resource_type values, unique URLs)
  - company_urls join table (many-to-many relationship)
  - website_snapshots (references url_id)
  - news_articles (references url_id)
  - Other supporting tables
- Implement many-to-many relationship between companies and URLs
- Create database initialization and migration scripts (Alembic)
- Write database access layer with proper connection pooling
- Add support for 23 resource types from Airtable
</tasks>
</step>

<step number="2">
  <title>Core Domain Models</title>
  <tasks>
    - Define Company model with has_urls flag and status including 'no_urls'
    - Define URL model with 23 resource_type enum values
    - Define CompanyURL join model for many-to-many relationships
    - Define WebsiteSnapshot model (linked to url_id only)
    - Define NewsArticle model (linked to url_id, related to companies via join table)
    - Define StatusDetermination model with signals
    - Define SnapshotChange model (per URL, not per company)
    - Define ProcessingResult and BatchSummary models
    - Create custom validators for URLs, dates, confidence scores, resource types
    - Implement resource type validation (23 supported types from Airtable)
    - Write comprehensive unit tests for all models including many-to-many scenarios
  </tasks>
</step>

<step number="3">
  <title>Airtable Integration</title>
  <tasks>
    - Implement Airtable API client with authentication
    - Fetch from 'Portfolio Companies' sheet (canonical company names)
    - Fetch from 'Online Presence' sheet (company_name, Resource, url)
    - Parse Resource field (23 types) into resource_type enum
    - Build many-to-many relationships via company_urls join table
    - Deduplicate URLs (same URL = one row in urls table, multiple companies via join)
    - Handle companies with zero URLs (create with status='no_urls', has_urls=false)
    - Implement data validation and normalization for each resource type
    - Handle missing or malformed URLs gracefully
    - Add retry logic for Airtable API calls
    - Read-only implementation (no writes back to Airtable)
    - Write integration tests with mocked Airtable responses
  </tasks>
</step>

<step number="4">
  <title>Website Scraping System</title>
  <tasks>
    - Implement Firecrawl API client using start_batch_scrape endpoint (NOT batch_scrape)
    - Implement batch processing: exactly 20 companies per batch
    - Implement Firecrawl search API for news articles with query pattern
    - CRITICAL: Implement Playwright scraper for ALL LinkedIn resources (Corporate and Person)
    - LinkedIn always requires Playwright - set requires_playwright=true by default
    - Create adaptive scraper selection logic:
      - Check URL.requires_playwright flag first
      - If false, try Firecrawl
      - On Firecrawl failure, switch to Playwright and set requires_playwright=true
      - Build learning list of problematic domains over time
    - Handle scraping of 23 resource types with appropriate scraper
    - Optional screenshot capture (configurable, especially for Twitter and LinkedIn)
    - Implement OCR (Tesseract) for extracting text from screenshots
    - Run keyword analysis on OCR-extracted text (Twitter image posts, LinkedIn graphics, etc.)
    - Store OCR text in database (ocr_text field)
    - Build link discovery for related pages (About, Blog, etc.) on homepage links
    - Implement platform-specific handling:
      - LinkedIn - Corporate: Check page existence, deactivation, recent updates
      - LinkedIn - Person: Check for role/company changes (CEO moved = bad sign)
      - Twitter/X: Check suspension, deletion, recent activity, capture screenshots if configured
      - Other platforms: Check removal, privacy changes, activity
    - Implement retry logic with exponential backoff (2 retries per URL)
    - Detect paywalls, authentication requirements, 404s, account suspensions
    - Extract HTTP headers (Last-Modified, ETag) where possible
    - Track which scraper was used (scraper_used field)
    - Handle edge cases (redirects, timeouts, SSL errors)
    - Gracefully handle companies with zero URLs (skip scraping, status='no_urls')
    - Parallel processing of multiple URLs per company (within batch limits)
    - Write unit and integration tests for Firecrawl and Playwright scrapers
  </tasks>
</step>

<step number="5">
  <title>News Search and Keyword Analysis</title>
  <tasks>
    - Implement Firecrawl search API with query pattern using company name and business description
    - Create article content extraction and cleaning
    - Build keyword detection system for positive/negative signals
    - Implement comprehensive keyword lists (70+ terms including investment vocabulary)
    - Implement keyword matching logic:
      - Minimum 2 matches required to flag
      - Track presence only (not frequency)
      - Handle mixed signals (2+ positive AND 2+ negative)
    - Store matched keywords in database for transparency
    - Implement relevance filtering to avoid false positives
    - Handle company name variations through business description context
    - Create article deduplication logic (many-to-many: same article can mention multiple companies)
    - Write tests with sample news articles and keyword matching scenarios
  </tasks>
</step>

<step number="6">
  <title>Snapshot Comparison Engine</title>
  <tasks>
    - Implement content normalization (strip CSS/styling, normalize whitespace)
    - Implement MD5 checksum comparison on normalized text content
    - Create change magnitude calculation (minor <10%, moderate 10-30%, major >30%)
    - All sections weighted equally (no special handling for footer, contact, etc.)
    - Build diff generation for changed content (text only)
    - Implement change history tracking (only store when content changed)
    - Create change summary generation
    - Add configurable thresholds for change classification
    - Write tests with various change scenarios including:
      - Text content changes (should detect)
      - CSS/styling changes only (should ignore)
      - Whitespace changes (should ignore)
      - Mixed changes (text + styling - should detect text only)
  </tasks>
</step>

<step number="7">
  <title>Status Analysis System</title>
  <tasks>
    - Implement dead website detection logic
    - Build acquisition mention detection
    - Create copyright staleness checker
    - Implement operational signal detection
    - Build confidence scoring algorithm
    - Create signal aggregation and weighting system
    - Generate human-readable status explanations
    - Write comprehensive tests for all signal types
  </tasks>
</step>

<step number="8">
  <title>Batch Processing Infrastructure</title>
  <tasks>
    - Design queue-based processing system
    - Implement worker pool with configurable concurrency
    - Build progress tracking with rich progress bars
    - Create checkpointing and resume functionality
    - Implement graceful shutdown (Ctrl+C handling)
    - Add rate limiting for API calls
    - Build processing statistics aggregation
    - Write batch processing tests
  </tasks>
</step>

<step number="9">
  <title>CLI Interface - Processing Commands</title>
  <tasks>
    - Choose CLI framework (click or typer)
    - Implement "process" command with options
    - Add dry-run mode for preview
    - Create verbose and quiet output modes
    - Build real-time progress display
    - Add interrupt handling
    - Implement configuration file loading
    - Write CLI integration tests
  </tasks>
</step>

<step number="10">
  <title>CLI Interface - Query Commands</title>
  <tasks>
    - Implement "query flagged" command
    - Create "query company" command with search
    - Build "query by-status" filtering
    - Implement "query changes" with date filters
    - Add "query search" with text matching
    - Format output as tables using rich library
    - Add pagination for large result sets
    - Write query command tests
  </tasks>
</step>

<step number="11">
  <title>Export and Reporting</title>
  <tasks>
    - Implement JSON export with configurable filters
    - Create CSV export with proper escaping
    - Build CLI summary report formatter
    - Add color-coded status output (green/yellow/red)
    - Generate processing statistics report
    - Create detailed error reports
    - Implement export command with options
    - Write export format tests
  </tasks>
</step>

<step number="12">
  <title>Configuration and Settings</title>
  <tasks>
    - Design configuration schema (TOML format)
    - Implement XDG directory support
    - Create default configuration file
    - Add environment variable support (API keys)
    - Implement config validation
    - Build "config check" command to verify setup
    - Document all configuration options
    - Write configuration tests
  </tasks>
</step>

<step number="13">
  <title>Database Utilities</title>
  <tasks>
    - Implement "db init" command
    - Create migration system for schema updates
    - Build "db clean" command to remove old snapshots
    - Implement "stats" command for database overview
    - Add database backup functionality
    - Create database integrity checks
    - Write database utility tests
  </tasks>
</step>

<step number="14">
  <title>Error Handling and Logging</title>
  <tasks>
    - Implement comprehensive error handlers
    - Create user-friendly error messages
    - Set up structured logging with context
    - Add debug logging mode
    - Implement log rotation and archival
    - Create error categorization and reporting
    - Test error scenarios thoroughly
  </tasks>
</step>

<step number="15">
  <title>Testing and Quality Assurance</title>
  <tasks>
    - Achieve 90%+ test coverage
    - Create integration test suite with real API calls (mocked)
    - Test with various company scenarios (active, closed, acquired)
    - Validate batch processing at scale (100+ companies)
    - Performance testing and optimization
    - Edge case testing (malformed data, network failures)
    - User acceptance testing with sample data
  </tasks>
</step>

<step number="16">
  <title>Documentation</title>
  <tasks>
    - Write comprehensive README with setup instructions
    - Create user guide with CLI examples
    - Document all configuration options
    - Write API integration guide (Airtable, Firecrawl)
    - Create troubleshooting guide
    - Document database schema
    - Write developer documentation
    - Add inline code documentation
  </tasks>
</step>

<step number="17">
  <title>Polish and Deployment</title>
  <tasks>
    - Final code review and refactoring
    - Performance optimization (database queries, API calls)
    - Clean up debug code and logging
    - Create package distribution setup
    - Write installation guide
    - Test on fresh environment
    - Create example configuration files
    - Prepare for production use
  </tasks>
</step>

</implementation_steps>

<success_criteria>
<functionality>
- Successfully fetch company data from multiple Airtable sheets
- Scrape websites with 95%+ success rate (excluding dead sites)
- Capture both HTML and Markdown content
- Detect changes between snapshots with high accuracy
- Search for news articles within past 2 weeks
- Accurately detect positive/negative keywords in news content
- Detect dead websites, stale content, and acquisitions
- Flag companies requiring human attention with ≥0.7 confidence
- Process 50-500 companies efficiently with batch processing
- Export results to JSON, CSV, and CLI reports
- Handle errors gracefully and continue batch processing
</functionality>

<quality_and_standards>
  - 90%+ test coverage across codebase
  - 100% coverage for functional core (business logic)
  - Zero mypy errors in strict mode
  - Zero ruff linting violations
  - All code follows SOLID principles
  - Clear FC/IS architectural separation
  - Comprehensive documentation
  - All configuration follows XDG standards
</quality_and_standards>

<performance>
  - Process 100 companies in under 10 minutes (with 10 workers)
  - Scrape typical homepage in under 5 seconds
  - Parallel processing with 10+ concurrent workers
  - Efficient database queries (&lt;100ms per query)
  - Resume interrupted runs without data loss
  - Minimal memory footprint (&lt;500MB for 100 companies)
  - Fast startup time (&lt;2 seconds)
</performance>

<user_experience>
  - Clear and intuitive CLI commands
  - Real-time progress tracking with ETA
  - Helpful error messages with troubleshooting hints
  - Color-coded output for easy scanning
  - Dry-run mode for safe previews
  - Easy configuration with sensible defaults
  - Works out of the box with API keys only
  - Professional formatted reports
</user_experience>

<data_accuracy>
  - &lt;5% false positives (operational companies flagged as problematic)
  - &lt;10% false negatives (problematic companies not flagged)
  - High confidence scores (&gt;0.8) for clear signals
  - Accurate change magnitude classification
  - Correct duplicate resolution across Airtable sheets
  - No data loss or corruption
  - Proper handling of edge cases
</data_accuracy>

</success_criteria>

<configuration_file_example>
<example_toml>
[airtable]
api_key_env = "AIRTABLE_API_KEY"
base_id_env = "AIRTABLE_BASE_ID"
sheet_names = ["Portfolio Companies", "Investments", "Prospects"]

  [firecrawl]
  api_key_env = "FIRECRAWL_API_KEY"
  endpoint = "start_batch_scrape"  # MUST use start_batch_scrape, NOT batch_scrape
  batch_size = 20  # Exactly 20 companies per batch (strict requirement)
  timeout = 30
  max_retries = 2
  retry_delay = 2.0

  [scraping]
  follow_links = true
  max_depth = 2
  respect_robots_txt = true
  capture_screenshots = false  # Optional, can be enabled per URL (especially Twitter)
  user_agent = "ValuationBot/1.0"
  adaptive_playwright = true  # Try Firecrawl first, switch to Playwright on failure

  [playwright]
  enabled = true
  headless = true
  timeout = 30000
  delay_between_actions = 1000  # ms delay to avoid detection

  [ocr]
  enabled = true
  engine = "tesseract"  # OCR engine to use
  language = "eng"  # Language code for OCR
  process_screenshots = true  # Run OCR on captured screenshots
  confidence_threshold = 60  # Minimum OCR confidence score (0-100)

  [news]
  enabled = true
  search_window_days = 90  # Use Firecrawl search API
  search_query_template = "Find articles about {company_name} ({business_description}) within the last {days} days. Prioritize articles from reputable sources. For each article identified, return the URL and the date it was published."
  max_articles_per_company = 10
  min_relevance_score = 0.6

  [keywords]
  # Comprehensive keyword lists for positive and negative signals
  positive_keywords = [
    # Funding
    "funding", "investment", "raised", "series A", "series B", "series C", "series D", "seed round", "angel investment", "venture capital", "VC backing", "oversubscribed", "unicorn", "valuation",
    # Product
    "launch", "launched", "new product", "product release", "announcing", "unveiling", "beta", "GA",
    # Growth
    "revenue", "growth", "profitable", "profitability", "milestone", "record sales", "doubled", "tripled", "ARR", "MRR",
    # Partnerships
    "partnership", "collaboration", "agreement", "strategic alliance", "joint venture", "signed deal",
    # Expansion
    "expansion", "expanding", "new office", "hiring", "jobs", "recruiting", "scale up", "scaling",
    # Recognition
    "award", "recognition", "winner", "top 10", "best of", "innovation award", "industry leader",
    # IPO
    "IPO", "going public", "public offering", "SPAC"
  ]

  negative_keywords = [
    # Closure
    "closed", "closing", "shutdown", "shut down", "ceased operations", "out of business", "discontinued", "winding down", "liquidation",
    # Layoffs
    "layoff", "layoffs", "laid off", "restructuring", "downsizing", "workforce reduction", "cutting staff", "job cuts", "RIF",
    # Financial
    "bankruptcy", "bankrupt", "insolvent", "Chapter 11", "Chapter 7", "cash crunch", "running out of money", "burn rate concerns", "down round",
    # Legal
    "lawsuit", "sued", "legal action", "regulatory action", "investigation", "probe", "fine", "penalty", "violation",
    # Security
    "breach", "security incident", "hack", "hacked", "compromised", "data leak", "cyberattack",
    # Acquisition
    "acquired by", "acquisition", "bought by", "purchased by", "merged with", "merger", "takeover", "sold to",
    # Leadership
    "CEO resigned", "CEO departure", "CFO resigned", "founder left", "executive departure", "stepping down", "ousted",
    # Product failure
    "recall", "failed", "discontinued product", "cancelled", "pulled from market", "defect",
    # Market exit
    "exiting market", "pulling out", "retreat", "abandoned", "pivoting away"
  ]

  minimum_keyword_matches = 2  # Require 2+ matches to flag article
  track_frequency = false  # Track presence only, not count
  weight_recent_articles = true

  [status_analysis]
  dead_website_signals = ["dns_failure", "404_error", "parking_page"]
  stale_copyright_year = 2024
  acquisition_keywords = ["acquired by", "merged with", "purchased by"]
  confidence_threshold = 0.7

  [change_detection]
  minor_threshold_percent = 10.0   # <10% = minor change
  moderate_threshold_percent = 30.0  # 10-30% = moderate change
  major_threshold_percent = 50.0   # >30% = major change
  ignore_cosmetic_changes = true   # Strip CSS/styling, compare text only
  normalize_whitespace = true      # Normalize whitespace before comparison
  equal_section_weighting = true   # All sections weighted equally

  [batch_processing]
  batch_size = 20  # Exactly 20 companies per batch for Firecrawl
  concurrency = 10  # Parallel workers within each batch
  checkpoint_interval = 1  # Checkpoint after each batch of 20
  skip_recent_snapshots = true
  recent_snapshot_hours = 24

  [output]
  cli_report = true
  json_export = false
  csv_export = false
  color_output = true

  [database]
  path = "~/.local/share/valuation-tool/companies.db"
  backup_on_startup = true
  retention_snapshots = 10

  [logging]
  level = "INFO"
  file = "~/.local/share/valuation-tool/logs/valuation.log"
  rotation = "daily"
  retention_days = 30
</example_toml>

</configuration_file_example>

<open_questions>
<airtable_structure>
RESOLVED:
- Sheets: 'Portfolio Companies' (canonical names) and 'Online Presence' (company_name, Resource, url)
- 23 Resource types supported (see airtable_integration section)
- Read-only access, all updates in SQLite
- Many-to-many relationship via join table
- Companies with no URLs get empty records with status='no_urls'
</airtable_structure>

<multi_link_handling>
RESOLVED:
  - Triage approach: ANY negative signal from ANY source triggers "requires_review"
  - Social media activity = positive sign; inactivity alone = neutral; inactivity + other issues = bad
  - Rate limiting handled by Firecrawl (and Playwright)
  - CRITICAL: Use Playwright for ALL LinkedIn scraping (Corporate and Person) - Firecrawl cannot access LinkedIn
  - Playwright as fallback for other social media if issues arise
  - Check LinkedIn - Person for bad signs (e.g., CEO has different company on LinkedIn = closure signal)
  - Conflicting signals (homepage dead but social active, or vice versa) → flag for review
  - Maximum 2 employee profiles per company - check all of them
  - Goal: identify companies needing human review, not make final determinations
</multi_link_handling>

<firecrawl_usage>
RESOLVED:
  - MUST use start_batch_scrape endpoint (NOT batch_scrape) for scraping
  - Use search API for finding news articles with query: "Find articles about [Company Name] ([business description]) within the last 90 days. Prioritize reputable sources. Return URL and date."
  - Batch size: exactly 20 companies at a time (strict requirement)
  - Add naics_code and business_description fields to companies table (to be implemented)
  - URLs table has requires_playwright flag for JavaScript-heavy sites
  - LinkedIn always requires Playwright (hard requirement)
  - Try Firecrawl first for other sites, flag for Playwright on failure (adaptive learning)
  - Screenshots optional, configurable per URL (especially Twitter)
  - Rate limiting handled by Firecrawl
</firecrawl_usage>

<keyword_configuration>
RESOLVED:
  - Comprehensive keyword lists including investment/startup vocabulary (see news_monitoring_system section)
  - No weighting - this is triage, any signal flags for review
  - Includes terms: unicorn, down round, bridge round, burn rate, runway, ARR, MRR, RIF, etc.
  - Minimum 2 keyword matches required to flag an article
  - Track presence only (not frequency) - found once = counted
  - Mixed signals (2+ positive AND 2+ negative) → flag for review
</keyword_configuration>

<change_magnitude_thresholds>
RESOLVED:
  - Minor: <10%, Moderate: 10-30%, Major: >30% content change
  - All sections weighted equally (no special handling)
  - Ignore cosmetic changes - compare text content only, strip CSS/styling
  - Normalize whitespace before comparison
  - Any detected change triggers review flag (triage approach)
</change_magnitude_thresholds>

<notification_system>
OUT OF SCOPE FOR V1:
  - No email/Slack notifications
  - No real-time alerts
  - No automated summary reports
  - Users will query database/CLI for results
  - Future enhancement: Can be added in later versions
</notification_system>

<future_enhancements>
OUT OF SCOPE FOR V1 (Future Versions):
  - Machine learning for improved status classification
  - Automated re-evaluation scheduling (cron job setup)
  - Web dashboard for viewing results
  - Support for non-English websites
  - OCR for images within web pages (beyond screenshots)

NOT NEEDED (Won't Implement):
  - CRM system integration (Salesforce, HubSpot, etc.)

IN SCOPE FOR V1:
  - OCR for screenshots captured by Playwright
  - Extract text from screenshot images (Twitter announcements, LinkedIn posts as images, etc.)
  - Run keyword analysis on OCR-extracted text
  - Store OCR-extracted text in database for future reference
</future_enhancements>

</open_questions>

<notes>
<project_status>
Currently in specification phase. No implementation started. Ready to begin development.
</project_status>

<critical_requirements>
  - MUST process companies from Airtable as data source (Portfolio Companies + Online Presence sheets)
  - MUST extract and store multiple URLs per company using many-to-many relationship
  - MUST handle companies with zero, few, or many URLs gracefully
  - MUST use many-to-many design: Companies ←→ CompanyURLs ←→ URLs
  - MUST support 23 resource types from Airtable
  - CRITICAL: MUST use start_batch_scrape endpoint (NOT batch_scrape) for Firecrawl
  - MUST process exactly 20 companies per batch (strict Firecrawl requirement)
  - MUST use Firecrawl search API for news discovery with company name and business description
  - CRITICAL: MUST use Playwright for ALL LinkedIn scraping (Corporate and Person) - Firecrawl cannot access LinkedIn
  - MUST use Firecrawl for most other resources (Homepage, Blog, Twitter, etc.)
  - MUST implement adaptive scraper selection: try Firecrawl first, flag for Playwright on failure
  - MUST support optional screenshot capture (especially for Twitter, LinkedIn)
  - MUST implement OCR (Tesseract) for extracting text from screenshots
  - MUST use keyword-based detection for analyzing news, website content, and OCR-extracted text
  - MUST store all data in SQLite database
  - MUST support batch processing for 50-500+ companies
  - MUST implement retry logic (2 retries per URL)
  - MUST detect dead websites, stale content, and acquisitions across all URL types
  - Triage approach: ANY negative signal from ANY source triggers "requires_review"
  - Goal: identify companies needing human review, not make final determinations
  - MUST aggregate signals from multiple URLs for company-wide status determination
  - MUST flag companies requiring human attention (over-flag rather than miss)
  - MUST provide confidence scores for determinations
  - MUST maintain historical snapshots indefinitely for all URLs
  - MUST follow XDG standards and src/ layout
  - 90% minimum test coverage required
</critical_requirements>

<design_decisions>
  - Language: Python 3.12+ (per user requirements)
  - Architecture: Functional Core / Imperative Shell
  - Database: SQLite (per requirements, no PostgreSQL)
  - Database schema: Many-to-many relationship (Companies ←→ CompanyURLs ←→ URLs)
  - Resource types: 23 types supported from Airtable Online Presence sheet
  - Scraping approach:
    - Playwright for ALL LinkedIn resources (REQUIRED - Firecrawl fails on LinkedIn)
    - Firecrawl for most other resources (handles rate limiting)
    - Playwright as fallback for social media if Firecrawl encounters issues
  - Triage philosophy: Better to over-flag than miss something needing review
  - Signal handling: ANY negative signal triggers "requires_review"
  - CLI only: No web interface in v1
  - Batch processing: Essential for medium-scale processing
  - Parallel processing: 10 workers default, configurable (parallelism per company for multiple URLs)
  - Storage: Indefinite retention of snapshots for all URLs
  - Export formats: JSON, CSV, CLI reports
  - Execution: Manual CLI runs (no automatic scheduling in v1)
  - Error handling: Graceful degradation when URLs are missing or inaccessible
  - Airtable access: Read-only (all updates in SQLite)
</design_decisions>

<development_approach>
  - Start with database schema and core models (TDD)
  - Build Airtable integration early for real data
  - Test scraping with sample companies
  - Iterate on status detection logic with real examples
  - Optimize batch processing for throughput
  - Polish CLI output for usability
  - Document as you build
</development_approach>

<technical_debt_to_avoid>
  - Avoid storing uncompressed HTML (can be large)
  - Don't ignore rate limits (will get blocked)
  - Don't skip validation on external data (Airtable, Firecrawl)
  - Don't hardcode thresholds (make configurable)
  - Don't use string concatenation for database queries (SQL injection)
  - Don't ignore connection pooling (performance impact)
</technical_debt_to_avoid>

</notes>
</project_specification>